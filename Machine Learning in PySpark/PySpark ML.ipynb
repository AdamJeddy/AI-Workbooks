{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in PySpark\n",
    "\n",
    "We will look at implementation of machine learning techniques such as logisitic regression in PySpark. \n",
    "The dataset used can be found on https://github.com/sam16tyagi/Machine-Learning-techniques-in-python/blob/master/logistic%20regression%20dataset-Social_Network_Ads.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Load your dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8de87ebb38>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"Social_Network_Ads.csv\", inferSchema=True, header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use print schema to show a summary of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User ID: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- EstimatedSalary: integer (nullable = true)\n",
      " |-- Purchased: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show the top 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---------------+---------+\n",
      "| User ID|Gender|Age|EstimatedSalary|Purchased|\n",
      "+--------+------+---+---------------+---------+\n",
      "|15624510|  Male| 19|          19000|        0|\n",
      "|15810944|  Male| 35|          20000|        0|\n",
      "+--------+------+---+---------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 \n",
    "Using Select() take all features except UserID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------------+---------+\n",
      "|Gender|Age|EstimatedSalary|Purchased|\n",
      "+------+---+---------------+---------+\n",
      "|  Male| 19|          19000|        0|\n",
      "|  Male| 35|          20000|        0|\n",
      "+------+---+---------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(\"Gender\",\"Age\", \"EstimatedSalary\", \"Purchased\")\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 \n",
    "Use a 70-30 ratio for train and testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = df2.randomSplit([0.3, 0.7], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120, 280]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[test.count(), train.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------------+---------+\n",
      "|Gender|Age|EstimatedSalary|Purchased|\n",
      "+------+---+---------------+---------+\n",
      "|Female| 18|          44000|        0|\n",
      "|Female| 19|          26000|        0|\n",
      "+------+---+---------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------------+---------+\n",
      "|Gender|Age|EstimatedSalary|Purchased|\n",
      "+------+---+---------------+---------+\n",
      "|Female| 18|          68000|        0|\n",
      "|Female| 18|          86000|        0|\n",
      "+------+---+---------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dtypes\n",
    "In this dataset, any column of type string is treated as a categorical feature, but sometimes we might have numeric features we want treated as categorical or vice versa. Weâ€™ll need to carefully identify which columns are numeric and which are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gender', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('EstimatedSalary', 'int'),\n",
       " ('Purchased', 'int')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding for categorical values\n",
    "\n",
    "StringIndexer:\n",
    "Converts a single feature to an index feature.\n",
    "http://spark.apache.org/docs/latest/ml-features#stringindexer\n",
    "\n",
    "OneHotEncoder:\n",
    "http://spark.apache.org/docs/latest/ml-features#onehotencoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'EstimatedSalary']\n",
      "['Gender']\n"
     ]
    }
   ],
   "source": [
    "#new \n",
    "\n",
    "catCols = [x for (x, dataType) in df2.dtypes if dataType == \"string\"]\n",
    "numCols = [ x for (x, dataType) in df2.dtypes if ((dataType == \"int\") & (x != \"Purchased\")) ]\n",
    "print(numCols)\n",
    "print(catCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(Gender)|\n",
      "+-------------+\n",
      "|            2|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.agg(F.countDistinct(\"Gender\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Gender|count|\n",
      "+------+-----+\n",
      "|Female|  204|\n",
      "|  Male|  196|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"Gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ______ OLD _______\n",
    "#catCols = [x for (x, dataType) in train.dtypes if dataType == \"string\"]\n",
    "#numCols = [\n",
    "#    x for (x, dataType) in train.dtypes if ((dataType == \"int\") & (x != \"Purchased\"))\n",
    "#]\n",
    "#print(numCols)\n",
    "#print(catCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.agg(F.countDistinct(\"Gender\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.groupBy(\"Gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder,\n",
    "    StringIndexer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = [\n",
    "    StringIndexer(inputCol=x, outputCol=x + \"_StringIndexer\", handleInvalid=\"skip\")\n",
    "    for x in catCols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_942a8b8d0c45]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = [\n",
    "    OneHotEncoder(\n",
    "        inputCols=[f\"{x}_StringIndexer\" for x in catCols],\n",
    "        outputCols=[f\"{x}_OneHotEncoder\" for x in catCols],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OneHotEncoder_f359c9e202b1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector assembling\n",
    "\n",
    "VectorAssembler:\n",
    "Combines the values of input columns into a single vector.\n",
    "http://spark.apache.org/docs/latest/ml-features#vectorassembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInput = [x for x in numCols]\n",
    "assemblerInput += [f\"{x}_OneHotEncoder\" for x in catCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age', 'EstimatedSalary', 'Gender_OneHotEncoder']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=assemblerInput, outputCol=\"VectorAssembler_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Stage together all your process \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [string_indexer[0], one_hot_encoder[0], vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_942a8b8d0c45,\n",
       " OneHotEncoder_f359c9e202b1,\n",
       " VectorAssembler_ad5737ca619f]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 ms, sys: 0 ns, total: 2.06 ms\n",
      "Wall time: 2.38 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 \n",
    "Describe the timings found above. Refer to pipeline documentation on spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System time is time spent running code on the OS kernal\n",
    "#  - 926 microseconds\n",
    "# CPU time is time spent on the processor running your \n",
    "# program's code\n",
    "#  - 495 microseconds\n",
    "# In total, sys + CPU Time is 1.42 milliseconds\n",
    "# Wall time is the elapsed real time of the code running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform model\n",
    "model = pipeline.fit(train)\n",
    "pp_df = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 \n",
    "Select the correct feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------------+------------------------+---------+\n",
      "|Age|EstimatedSalary|Gender_OneHotEncoder|VectorAssembler_features|Purchased|\n",
      "+---+---------------+--------------------+------------------------+---------+\n",
      "| 18|          44000|       (1,[0],[1.0])|      [18.0,44000.0,1.0]|        0|\n",
      "| 19|          26000|       (1,[0],[1.0])|      [19.0,26000.0,1.0]|        0|\n",
      "| 22|          27000|       (1,[0],[1.0])|      [22.0,27000.0,1.0]|        0|\n",
      "| 24|          55000|       (1,[0],[1.0])|      [24.0,55000.0,1.0]|        0|\n",
      "| 26|          15000|       (1,[0],[1.0])|      [26.0,15000.0,1.0]|        0|\n",
      "| 26|          43000|       (1,[0],[1.0])|      [26.0,43000.0,1.0]|        0|\n",
      "| 26|          52000|       (1,[0],[1.0])|      [26.0,52000.0,1.0]|        0|\n",
      "| 26|          80000|       (1,[0],[1.0])|      [26.0,80000.0,1.0]|        0|\n",
      "| 28|          59000|       (1,[0],[1.0])|      [28.0,59000.0,1.0]|        0|\n",
      "| 28|          84000|       (1,[0],[1.0])|      [28.0,84000.0,1.0]|        0|\n",
      "| 28|          85000|       (1,[0],[1.0])|      [28.0,85000.0,1.0]|        0|\n",
      "| 29|          28000|       (1,[0],[1.0])|      [29.0,28000.0,1.0]|        0|\n",
      "| 29|          47000|       (1,[0],[1.0])|      [29.0,47000.0,1.0]|        0|\n",
      "| 30|          79000|       (1,[0],[1.0])|      [30.0,79000.0,1.0]|        0|\n",
      "| 31|          34000|       (1,[0],[1.0])|      [31.0,34000.0,1.0]|        0|\n",
      "| 31|          89000|       (1,[0],[1.0])|      [31.0,89000.0,1.0]|        0|\n",
      "| 32|          86000|       (1,[0],[1.0])|      [32.0,86000.0,1.0]|        0|\n",
      "| 32|         117000|       (1,[0],[1.0])|     [32.0,117000.0,1.0]|        1|\n",
      "| 33|          60000|       (1,[0],[1.0])|      [33.0,60000.0,1.0]|        0|\n",
      "| 33|         149000|       (1,[0],[1.0])|     [33.0,149000.0,1.0]|        1|\n",
      "+---+---------------+--------------------+------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = pp_df.select(\"Age\", \"EstimatedSalary\", \"Gender_OneHotEncoder\", \"VectorAssembler_features\", \"Purchased\")\n",
    "temp.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Select and assemble your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pp_df.selectExpr(\"VectorAssembler_features as features\", \"Purchased as label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|features          |label|\n",
      "+------------------+-----+\n",
      "|[18.0,44000.0,1.0]|0    |\n",
      "|[19.0,26000.0,1.0]|0    |\n",
      "|[22.0,27000.0,1.0]|0    |\n",
      "|[24.0,55000.0,1.0]|0    |\n",
      "|[26.0,15000.0,1.0]|0    |\n",
      "+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 ms, sys: 14.6 ms, total: 44 ms\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression().fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362026862026862"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|              recall|         precision|\n",
      "+--------------------+------------------+\n",
      "|                 0.0|               1.0|\n",
      "|0.023809523809523808|               1.0|\n",
      "|0.047619047619047616|               1.0|\n",
      "| 0.07142857142857142|               1.0|\n",
      "| 0.09523809523809523|               1.0|\n",
      "| 0.11904761904761904|               1.0|\n",
      "| 0.14285714285714285|               1.0|\n",
      "| 0.16666666666666666|               1.0|\n",
      "| 0.19047619047619047|               1.0|\n",
      "| 0.21428571428571427|               1.0|\n",
      "| 0.23809523809523808|               1.0|\n",
      "|  0.2619047619047619|               1.0|\n",
      "|  0.2857142857142857|               1.0|\n",
      "| 0.30952380952380953|               1.0|\n",
      "|  0.3333333333333333|               1.0|\n",
      "| 0.35714285714285715|               1.0|\n",
      "| 0.38095238095238093|               1.0|\n",
      "| 0.38095238095238093|0.9411764705882353|\n",
      "| 0.40476190476190477|0.9444444444444444|\n",
      "| 0.42857142857142855|0.9473684210526315|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.summary.pr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 \n",
    "Obtain the confusion matrix of the classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85 \n",
      "Precision: 0.8483311938382541\n",
      "Recall: 0.8500000000000001\n",
      "\n",
      "False Positive Rate: 0.20164835164835165\n",
      "True Positive Rate: 0.8500000000000001\n",
      "F-measure: 0.848125\n"
     ]
    }
   ],
   "source": [
    "Summary = model.summary\n",
    "\n",
    "accuracy = Summary.accuracy\n",
    "falsePositiveRate = Summary.weightedFalsePositiveRate\n",
    "truePositiveRate = Summary.weightedTruePositiveRate\n",
    "fMeasure = Summary.weightedFMeasure()\n",
    "precision = Summary.weightedPrecision\n",
    "recall = Summary.weightedRecall\n",
    "print(\"Accuracy: %s \\nPrecision: %s\\nRecall: %s\\n\\nFalse Positive Rate: %s\\nTrue Positive Rate: %s\\nF-measure: %s\"\n",
    "      % (accuracy, precision, recall, falsePositiveRate, truePositiveRate, fMeasure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
